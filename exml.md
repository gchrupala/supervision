# Explainable ML and analysis methods for neural networks

## Description
Machine learning in general and neural network models in particular are being used in an increasingly wide variety of domains, impacting society in deep but often poorly understood ways. As such, interest in developing methods which allow us to better understand these models has been growing. Among others, the following research problems have been addressed:

- Probing neural activation patterns to find out what information they decode (aka diagnostic classifiers)
- Visualizing input features influencing model decisions
- Modifying neural architectures to make them more interpretable
- Applying neural models to synthetic data to understand their performance in simplified settings

## Requirements: Solid Python programming skills and familiarity with machine learning and neural networks specifically. 
For this project you will need to find a concrete topic related to one or more of the points listed above, and explore it experimentally on datasets of your choice. Especially welcome are topics related to processing natural language or speech. 

## References:

## Websites
- BlackboxNLP workshop series: https://blackboxnlp.github.io/
- Interpreting Deep Learning Models for Text and Sound: Methods & Applications. https://interpretingdl.github.io/

### Surveys
- Christoph Molnar (2019). Interpretable Machine Learning: A Guide for Making Black Box Models Explainable. https://christophm.github.io/interpretable-ml-book/
- Wojciech Samek, Thomas Wiegand & Klaus-Robert Müller (2017). Explainable Artificial Intelligence: Understanding, Visualizing and Interpreting Deep Learning Models. ITU Journal: ICT Discoveries, Special Issue No. 1, 13. https://www.itu.int/dms_pub/itu-s/opb/journal/S-JOURNAL-ICTF.VOL1-2018-1-P05-PDF-E.pdf
- Belinkov, Y., & Glass, J. (2019). Analysis methods in neural language processing: A survey. Transactions of the Association for Computational Linguistics, 7, 49-72. https://www.mitpressjournals.org/doi/full/10.1162/tacl_a_00254
- Afra Alishahi, Grzegorz Chrupala, & Tal Linzen (2019). Analyzing and Interpreting Neural Networks for NLP: A Report on the First BlackboxNLP Workshop. Natural Language Engineering, 25(4), 543-557. http://dx.doi.org/10.1017/S135132491900024X, preprint: https://arxiv.org/abs/1904.04063 

### Research papers

- Hewitt, J., & Manning, C. D. (2019). A structural probe for finding syntax in word representations. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers) (pp. 4129-4138). https://www.aclweb.org/anthology/N19-1419.pdf
- Grzegorz Chrupała & Afra Alishahi (2019). Correlating neural and symbolic representations of language. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. https://www.aclweb.org/anthology/P19-1283/
- Giulianelli, M., Harding, J., Mohnert, F., Hupkes, D., & Zuidema, W. (2018). Under the Hood: Using Diagnostic Classifiers to Investigate and Improve how Language Models Track Agreement Information. In Proceedings EMNLP workshop Analyzing and interpreting neural networks for NLP (BlackboxNLP). https://www.aclweb.org/anthology/W18-5426/
- Hupkes, D., Veldhoen, S., & Zuidema, W. (2018). Visualisation and ‘Diagnostic Classifiers’ reveal how recurrent and recursive neural networks process hierarchical structure. Journal of Artificial Intelligence Research, 61, 907–926. https://dl.acm.org/doi/10.5555/3241691.3241713
- Alishahi, A., Barking, M., & Chrupała, G. (2017, August). Encoding of phonology in a recurrent neural model of grounded speech. In Proceedings of the 21st Conference on Computational Natural Language Learning (CoNLL 2017) (pp. 368-378). https://www.aclweb.org/anthology/K17-1037.pdf
- Chrupała, G., Gelderloos, L., & Alishahi, A. (2017, July). Representations of language in a model of visually grounded speech signal. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) (pp. 613-622). https://www.aclweb.org/anthology/P17-1057.pdf
- Ákos Kádár, Grzegorz Chrupała and Afra Alishahi (2017). Representation of linguistic form and function in recurrent neural networks. Computational Linguistics, 43(4):761-780. https://www.mitpressjournals.org/doi/abs/10.1162/COLI_a_00300
- Linzen, T., Dupoux, E., & Goldberg, Y. (2016). Assessing the ability of LSTMs to learn syntax-sensitive dependencies. Transactions of the Association for Computational Linguistics, 4, 521-535. https://www.mitpressjournals.org/doi/abs/10.1162/tacl_a_00115
Ribeiro, M. T., Singh, S., & Guestrin, C. (2016). Model-agnostic interpretability of machine learning. arXiv preprint arXiv:1606.05386. https://arxiv.org/abs/1606.05386
- Matthew D Zeiler & Rob Fergus (2014). Visualizing and understanding convolutional networks. In European Conference on Computer Vision (pp. 818-833). https://arxiv.org/pdf/1311.2901
